{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIjdmWUNSfiP"
      },
      "source": [
        "# Fully connected neural network (FCNN)\n",
        "\n",
        "In this notebook, we will use a fully-connected neural network and physicochemical descriptors to predict the value of blood brain barrier (BBB) penetration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install conda\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "C8B27IuPSiq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check conda installation\n",
        "import condacolab\n",
        "condacolab.check()\n",
        "\n",
        "# Install required packages\n",
        "!mamba install python=3 pip\n",
        "!mamba install -c conda-forge pandas numpy matplotlib rdkit scikit-learn\n",
        "!pip3 install torch --index-url https://download.pytorch.org/whl/cu126\n",
        "\n",
        "# Download required files\n",
        "!wget https://github.com/lillgroup/AIiDD/raw/main/Lab%202/Data/model_saved.pt\n",
        "!wget https://github.com/lillgroup/AIiDD/raw/main/Lab%202/Data/B3DB_regression.tsv"
      ],
      "metadata": {
        "id": "nJgXSgybS1Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3L8qUkuSfiR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import Descriptors, Crippen, AllChem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S80i-GYeSfiS"
      },
      "source": [
        "There are some Warnings informing about future scipy changes.\n",
        "\n",
        "We will suppress them here as they have no influence on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKcdIhksSfiS"
      },
      "outputs": [],
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk57tMnSSfiS"
      },
      "source": [
        "### Read data\n",
        "\n",
        "Here we use classification data on blood brain barrier (BBB) penetration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0PR4FbTSfiS"
      },
      "outputs": [],
      "source": [
        "tmp = pd.read_table('B3DB_regression.tsv',sep='\\t')\n",
        "\n",
        "table = tmp.loc[:,('compound_name', 'IUPAC_name', 'SMILES', 'logBB')]\n",
        "table = table.dropna(subset=\"logBB\")\n",
        "table.reset_index(drop=True, inplace=True)\n",
        "\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urIMz25LSfiS"
      },
      "source": [
        "### Molecular descriptors calculation\n",
        "\n",
        "We can use RDKIT to calculate several molecular descriptors (2D and 3D)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEoaI3cESfiS"
      },
      "outputs": [],
      "source": [
        "# We will calculate the descriptors and add them to our table\n",
        "for i in table.index:\n",
        "    mol=Chem.MolFromSmiles(table.loc[i,'SMILES'])\n",
        "    table.loc[i,'MolWt']=Descriptors.ExactMolWt (mol)\n",
        "    table.loc[i,'TPSA']=Chem.rdMolDescriptors.CalcTPSA(mol) #Topological Polar Surface Area\n",
        "    table.loc[i,'nRotB']=Descriptors.NumRotatableBonds (mol) #Number of rotable bonds\n",
        "    table.loc[i,'HBD']=Descriptors.NumHDonors(mol) #Number of H bond donors\n",
        "    table.loc[i,'HBA']=Descriptors.NumHAcceptors(mol) #Number of H bond acceptors\n",
        "    table.loc[i,'LogP']=Descriptors.MolLogP(mol) #LogP\n",
        "\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-PD5-jFSfiS"
      },
      "source": [
        "### Model: We will use neural network regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeuMXaGYSfiT"
      },
      "source": [
        "Generate x (descriptors) and y (logBB) vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW-_2h1OSfiT"
      },
      "outputs": [],
      "source": [
        "descriptors_selected = ['MolWt', 'TPSA', 'nRotB', 'HBD', 'HBA', 'LogP']\n",
        "\n",
        "x = table.loc[:, descriptors_selected].values\n",
        "y = table.loc[:, ['logBB']].values\n",
        "\n",
        "# standardization/normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x = scaler.fit_transform(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHiqrOT9SfiT"
      },
      "source": [
        "### Split data into training and test set\n",
        "\n",
        "Here, we use random splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRrSc8TJSfiT"
      },
      "outputs": [],
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1)\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0B8HU6iSfiT"
      },
      "source": [
        "### Baseline: Multilinear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0srfI_ZSfiT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "reg = LinearRegression().fit(x_train, y_train)\n",
        "reg.score(x_train, y_train)\n",
        "pred_train = reg.predict(x_train)\n",
        "pred_valid = reg.predict(x_valid)\n",
        "pred_test = reg.predict(x_test)\n",
        "\n",
        "# Mean squared error\n",
        "print(\"Mean squared error, train: %.2f\" % mean_squared_error(y_train, pred_train))\n",
        "print(\"Mean squared error, valid: %.2f\" % mean_squared_error(y_valid, pred_valid))\n",
        "print(\"Mean squared error, test : %.2f\" % mean_squared_error(y_test, pred_test))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination, train: %.2f\" % r2_score(y_train, pred_train))\n",
        "print(\"Coefficient of determination, valid: %.2f\" % r2_score(y_valid, pred_valid))\n",
        "print(\"Coefficient of determination, test: %.2f\" % r2_score(y_test, pred_test))\n",
        "\n",
        "# Plot outputs\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(y_train, pred_train, color=\"black\", s=10)\n",
        "plt.scatter(y_valid, pred_valid, color=\"lightgreen\", s=10)\n",
        "plt.scatter(y_test, pred_test, color=\"red\", s=10)\n",
        "\n",
        "\n",
        "plt.plot([-3.0, 2.0], [-3.0, 2.0], color=\"black\", linewidth=3)\n",
        "\n",
        "plt.xlabel(\"ground truth\")\n",
        "plt.ylabel(\"predicted\")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwjkV4CmSfiT"
      },
      "source": [
        "### Generate Dataset for pytorch\n",
        "\n",
        "We generate three instances (training, validation and test set) of a class Data that is derived from the pytorch class Dataset.\n",
        "\n",
        "Then, we initiate three dataloaders for the three instances that will be used to provide data to the training, validation and test phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grnT6PDGSfiT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "# Convert data to torch tensors\n",
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))\n",
        "        self.len = self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Instantiate training, validation and test data\n",
        "train_data = Data(x_train, y_train)\n",
        "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_data = Data(x_valid, y_valid)\n",
        "valid_dataloader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = Data(x_test, y_test)\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Check batches\n",
        "for loaders in [train_dataloader, valid_dataloader, test_dataloader]:\n",
        "    print(\"------------------------------------------------------------\")\n",
        "    for batch, (X, y) in enumerate(loaders):\n",
        "        print(f\"Batch: {batch+1}\")\n",
        "        print(f\"X shape: {X.shape}\")\n",
        "        print(f\"y shape: {y.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdiN-E03SfiU"
      },
      "source": [
        "### Network architecture\n",
        "\n",
        "We implement a simple two-layer neural network that uses ReLU activation.\n",
        "\n",
        "The architecture is defined by the class NeuralNetwork that that is derivec from pytorch's nn.Module which is the base class for all neural network modules built in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DdphDSqSfiU"
      },
      "outputs": [],
      "source": [
        "# Number of input descriptors per sample (need to be changed if you use other input)\n",
        "input_dim = 6\n",
        "# Size if hidden layer\n",
        "hidden_dim = 64\n",
        "# Output is one float value representing logBB\n",
        "output_dim = 1\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
        "        nn.init.xavier_uniform_(self.layer_1.weight)\n",
        "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
        "        nn.init.xavier_uniform_(self.layer_2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "print('# of model parameters:', sum([np.prod(p.size()) for p in model.parameters()]))\n",
        "for p in model.parameters():\n",
        "    print(p.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ehpgGHtSfiU"
      },
      "source": [
        "### Optimizer and loss function\n",
        "\n",
        "To train the model we define a loss function to calculate the gradients and an optimizer to update the parameters.\n",
        "\n",
        "Here we use mean square error (MSE) and the ADAM optimizer with a learning rate of 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULrZn9KjSfiU"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLVobn2xSfiU"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNEMzwxHSfiU"
      },
      "outputs": [],
      "source": [
        "num_epochs = 500\n",
        "loss_values = []\n",
        "loss_epoch_train = []\n",
        "loss_epoch_valid = []\n",
        "\n",
        "count_batches = 0\n",
        "for epoch in range(num_epochs):\n",
        "    loss_epoch = 0\n",
        "    for X, y in train_dataloader:\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # save loss values\n",
        "        count_batches += 1\n",
        "        loss_values.append([count_batches, loss.item()])\n",
        "        loss_epoch += X.shape[0]*loss.item()\n",
        "\n",
        "        # optimize model\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # save loss for each epoch\n",
        "    loss_epoch_train.append([count_batches, loss_epoch/train_data.X.shape[0]])\n",
        "    # validation loss\n",
        "    # TO DO\n",
        "\n",
        "torch.save(model, \"model_saved.pt\")\n",
        "\n",
        "print(\"Training Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7vQrjTrSfiU"
      },
      "source": [
        "#### Plot training loss as function of number of batches (here: One epoch = 10 batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0HqaiZ4SfiU"
      },
      "outputs": [],
      "source": [
        "step = np.linspace(0, num_epochs, num_epochs*10)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,5))\n",
        "#plt.plot(step, np.array(loss_values))\n",
        "loss_values_np = np.asanyarray(loss_values)\n",
        "loss_epoch_train_np = np.asanyarray(loss_epoch_train)\n",
        "plt.plot(loss_values_np[:,0], loss_values_np[:,1], \"-\")\n",
        "plt.plot(loss_epoch_train_np[:,0], loss_epoch_train_np[:,1], \"-\", c=\"r\")\n",
        "plt.title(\"Step-wise Loss\")\n",
        "plt.xlabel(\"Batches\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lMervyOSfiU"
      },
      "source": [
        "#### Correlation between experimental and predicted logBB for training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTwCtoptSfiU"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"model_saved.pt\", weights_only=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_train = model(train_data.X).detach().numpy()\n",
        "\n",
        "# Mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_train, model(train_data.X).detach().numpy()))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_train, model(train_data.X).detach().numpy()))\n",
        "\n",
        "# Plot outputs\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(y_train, model(train_data.X).detach().numpy(), color=\"black\", s=10)\n",
        "plt.plot([-3.0, 2.0], [-3.0, 2.0], color=\"black\", linewidth=3)\n",
        "\n",
        "plt.xlabel(\"ground truth\")\n",
        "plt.ylabel(\"predicted\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQCJBh_bSfiU"
      },
      "source": [
        "#### Correlation between experimental and predicted logBB for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s9KUyU-SfiU"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    pred_test = model(test_data.X).detach().numpy()\n",
        "\n",
        "# Mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, model(test_data.X).detach().numpy()))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, model(test_data.X).detach().numpy()))\n",
        "\n",
        "# Plot outputs\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(y_test, model(test_data.X).detach().numpy(), color=\"red\", s=10)\n",
        "plt.plot([-3.0, 2.0], [-3.0, 2.0], color=\"black\", linewidth=3)\n",
        "\n",
        "plt.xlabel(\"ground truth\")\n",
        "plt.ylabel(\"predicted\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NSXivuWSfiV"
      },
      "source": [
        "#### Comparison between training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5VOTitLSfiV"
      },
      "outputs": [],
      "source": [
        "step = np.linspace(0, num_epochs, num_epochs*10)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,5))\n",
        "#plt.plot(step, np.array(loss_values))\n",
        "loss_epoch_train_np = np.asanyarray(loss_epoch_train)\n",
        "loss_epoch_valid_np = np.asanyarray(loss_epoch_valid)\n",
        "plt.plot(loss_epoch_train_np[:,0]/10, loss_epoch_train_np[:,1], \"-\", c=\"r\")\n",
        "plt.plot(loss_epoch_valid_np[:,0]/10, loss_epoch_valid_np[:,1], \"-\", c=\"g\")\n",
        "plt.title(\"Step-wise Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.9 ('nn_ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "67d2b4b599ae8ea1e8d006974eef039fc80f58295d657378e7c0e6af2491ee8a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}